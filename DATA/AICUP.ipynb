{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e99bb7fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.22.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (4.22.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from transformers==4.22.0) (0.19.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.22.0) (1.26.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from transformers==4.22.0) (0.12.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\user\\miniconda3\\lib\\site-packages (from transformers==4.22.0) (4.65.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\user\\miniconda3\\lib\\site-packages (from transformers==4.22.0) (2023.10.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from transformers==4.22.0) (23.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\miniconda3\\lib\\site-packages (from transformers==4.22.0) (2.28.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\miniconda3\\lib\\site-packages (from transformers==4.22.0) (3.12.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from transformers==4.22.0) (6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22.0) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\miniconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.9.0->transformers==4.22.0) (4.6.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\lib\\site-packages (from tqdm>=4.27->transformers==4.22.0) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests->transformers==4.22.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests->transformers==4.22.0) (2023.7.22)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests->transformers==4.22.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests->transformers==4.22.0) (1.26.15)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1f799fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\user\\miniconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: xxhash in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (3.4.1)\n",
      "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.18.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (0.19.4)\n",
      "Requirement already satisfied: pyarrow-hotfix in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (0.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (14.0.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (23.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (2.0.3)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (3.8.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from datasets) (1.26.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from datasets) (2.28.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\user\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.9.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\user\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (4.0.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\miniconda3\\lib\\site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\miniconda3\\lib\\site-packages (from huggingface-hub>=0.18.0->datasets) (4.6.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from requests>=2.19.0->datasets) (1.26.15)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\miniconda3\\lib\\site-packages (from tqdm>=4.62.1->datasets) (0.4.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas->datasets) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\appdata\\roaming\\python\\python310\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\user\\miniconda3\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b29c48ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig, get_linear_schedule_with_warmup\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c73cdcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: islab-opendeid in c:\\users\\user\\miniconda3\\lib\\site-packages (0.0.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install islab-opendeid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5f1acbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_torch_seed(seed=0):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benckmark = False\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "set_torch_seed()\n",
    "\n",
    "def read_file(path):\n",
    "    with open(path , 'r' , encoding = 'utf-8-sig') as fr:\n",
    "        return fr.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7d45b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process annotation file...\n",
      "annotation file done\n",
      "processing each medical file\n",
      "['10\\t25\\tEpisode No:  09F016547J\\tIDNUM:09F016547J\\n', '10\\t36\\t091016.NMT\\tMEDICALRECORD:091016.NMT\\n', '10\\t52\\tSIZAR, HOWARD\\tPATIENT:SIZAR, HOWARD\\n', '10\\t70\\tLab No:  09F01654\\tIDNUM:09F01654\\n', '10\\t78\\tRunford\\tSTREET:Runford\\n', '10\\t97\\tRENMARK  TAS  5084\\tCITY:RENMARK\\\\nSTATE:TAS\\\\nZIP:5084\\n', '10\\t114\\tSpecimen: Tissue\\tPHI:Null\\n', '10\\t132\\tD.O.B:  24/8/1993\\tDATE:24/8/1993=>1993-08-24\\n', '10\\t140\\tSex:  M\\tPHI:Null\\n', '10\\t171\\tCollected: 28/08/2013 at 08:26\\tTIME:28/08/2013 at 08:26=>2013-08-28T08:26\\n']\n",
      "All medical file done\n",
      "write out to tsv format...\n",
      "tsv format dataset done\n"
     ]
    }
   ],
   "source": [
    "bos = '<|endoftext|>'\n",
    "eos = '<|END|>'\n",
    "pad = '<|pad|>'\n",
    "ner = '\\n\\n####\\n\\n'\n",
    "special_tokens_dict = {'bos_token': bos,\n",
    "                       'eos_token': eos,\n",
    "                       'pad_token': pad,\n",
    "                       'sep_token': ner}\n",
    "\n",
    "def process_annotation_file(lines):\n",
    "    '''\n",
    "    處理anwser.txt 標註檔案\n",
    "\n",
    "    output:annotation dicitonary\n",
    "    '''\n",
    "    print(\"process annotation file...\")\n",
    "    entity_dict = {}\n",
    "    for line in lines:\n",
    "        items = line.strip('\\n').split('\\t')\n",
    "        if len(items) == 5:\n",
    "            item_dict = {\n",
    "                'phi' : items[1],\n",
    "                'st_idx' : int(items[2]),\n",
    "                'ed_idx' : int(items[3]),\n",
    "                'entity' : items[4],\n",
    "            }\n",
    "        elif len(items) == 6:\n",
    "            item_dict = {\n",
    "                'phi' : items[1],\n",
    "                'st_idx' : int(items[2]),\n",
    "                'ed_idx' : int(items[3]),\n",
    "                'entity' : items[4],\n",
    "                'normalize_time' : items[5],\n",
    "            }\n",
    "        if items[0] not in entity_dict:\n",
    "            entity_dict[items[0]] = [item_dict]\n",
    "        else:\n",
    "            entity_dict[items[0]].append(item_dict)\n",
    "    print(\"annotation file done\")\n",
    "    return entity_dict\n",
    "\n",
    "def process_medical_report(txt_name, medical_report_folder, annos_dict, special_tokens_dict):\n",
    "    '''\n",
    "    處理單個病理報告\n",
    "\n",
    "    output : 處理完的 sequence pairs\n",
    "    '''\n",
    "    file_name = txt_name + '.txt'\n",
    "    sents = read_file(os.path.join(medical_report_folder, file_name))\n",
    "    #print(sents)\n",
    "    article = \"\".join(sents)\n",
    "    #print(article[0:100])\n",
    "    \n",
    "    #for w_idx, word in enumerate(article):\n",
    "        #if w_idx == 1:\n",
    "            #print(word)\n",
    "    \n",
    "    \n",
    "\n",
    "    bounary , item_idx , temp_seq , seq_pairs = 0 , 0 , \"\" , []\n",
    "    new_line_idx = 0\n",
    "    for w_idx, word in enumerate(article):\n",
    "        if word == '\\n':\n",
    "            new_line_idx = w_idx + 1\n",
    "            if article[bounary:new_line_idx] == '\\n':\n",
    "                continue\n",
    "            if temp_seq == \"\":\n",
    "                temp_seq = \"PHI:Null\"\n",
    "            sentence = article[bounary:new_line_idx].strip().replace('\\t' , ' ')\n",
    "            temp_seq = temp_seq.strip('\\\\n')\n",
    "            seq_pair = f\"{txt_name}\\t{new_line_idx}\\t{sentence}\\t{temp_seq}\\n\"\n",
    "            # seq_pair = special_tokens_dict['bos_token'] + article[bounary:new_line_idx] + special_tokens_dict['sep_token'] + temp_seq + special_tokens_dict['eos_token']\n",
    "            bounary = new_line_idx\n",
    "            seq_pairs.append(seq_pair)\n",
    "            temp_seq = \"\"\n",
    "        if w_idx == annos_dict[txt_name][item_idx]['st_idx']:\n",
    "            phi_key = annos_dict[txt_name][item_idx]['phi']\n",
    "            phi_value = annos_dict[txt_name][item_idx]['entity']\n",
    "            if 'normalize_time' in annos_dict[txt_name][item_idx]:\n",
    "                temp_seq += f\"{phi_key}:{phi_value}=>{annos_dict[txt_name][item_idx]['normalize_time']}\\\\n\"\n",
    "            else:\n",
    "                temp_seq += f\"{phi_key}:{phi_value}\\\\n\"\n",
    "            if item_idx == len(annos_dict[txt_name]) - 1:\n",
    "                continue\n",
    "            item_idx += 1\n",
    "    return seq_pairs\n",
    "\n",
    "def generate_annotated_medical_report_parallel(anno_file_path, medical_report_folder , tsv_output_path , num_processes=4):\n",
    "    '''\n",
    "    呼叫上面的兩個function\n",
    "    處理全部的病理報告和標記檔案\n",
    "\n",
    "    output : 全部的 sequence pairs\n",
    "    '''\n",
    "    anno_lines = read_file(anno_file_path)\n",
    "    #print(anno_lines)\n",
    "    annos_dict = process_annotation_file(anno_lines)\n",
    "    #print(annos_dict)\n",
    "    txt_names = list(annos_dict.keys())\n",
    "    #print(txt_names)\n",
    "    \n",
    "    print(\"processing each medical file\")\n",
    "\n",
    "    all_seq_pairs = []\n",
    "    for txt_name in txt_names:\n",
    "        all_seq_pairs.extend(process_medical_report(txt_name, medical_report_folder, annos_dict, special_tokens_dict))\n",
    "    print(all_seq_pairs[:10])\n",
    "    print(\"All medical file done\")\n",
    "    print(\"write out to tsv format...\")\n",
    "    with open(tsv_output_path , 'w' , encoding = 'utf-8') as fw:\n",
    "        for seq_pair in all_seq_pairs:\n",
    "            fw.write(seq_pair)\n",
    "    print(\"tsv format dataset done\")\n",
    "    # return all_seq_pairs\n",
    "\n",
    "anno_info_path = r\"C:\\Users\\user\\OneDrive\\Documents\\資料探勘\\DATA\\First_Phase_Release(Correction)/answer.txt\"\n",
    "report_folder = r\"C:\\Users\\user\\OneDrive\\Documents\\資料探勘\\DATA\\First_Phase_Release(Correction)/First_Phase_Text_Dataset\"\n",
    "tsv_output_path = './train.tsv'\n",
    "generate_annotated_medical_report_parallel(anno_info_path, report_folder, tsv_output_path, num_processes=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46231d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e7da816bba45acb25684b64db54ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c37b702c7a2413bba27f59d6cbaf971",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "580395a2b99d41208e68e109ea3fb7fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset, Features, Value\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files=\"./train.tsv\", delimiter='\\t',\n",
    "                       features = Features({\n",
    "                              'fid': Value('string'), 'idx': Value('int64'),\n",
    "                              'content': Value('string'), 'label': Value('string')}),\n",
    "                              column_names=['fid', 'idx', 'content', 'label'], keep_default_na=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a296ae3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fid': '104',\n",
       " 'idx': 110,\n",
       " 'content': 'MOURA  WA  2671',\n",
       " 'label': 'CITY:MOURA\\\\nSTATE:WA\\\\nZIP:2671'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "968c5b3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53213"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f7d7f25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|pad|>: 50278\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "plm = \"EleutherAI/pythia-70m\" #\"EleutherAI/pythia-70m-deduped\"\n",
    "\n",
    "bos = '<|endoftext|>'\n",
    "eos = '<|END|>'\n",
    "pad = '<|pad|>'\n",
    "sep ='\\n\\n####\\n\\n'\n",
    "\n",
    "special_tokens_dict = {'eos_token': eos, 'bos_token': bos, 'pad_token': pad, 'sep_token': sep}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(plm, revision=\"step3000\")\n",
    "tokenizer.padding_side = 'left'\n",
    "num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n",
    "print(f\"{tokenizer.pad_token}: {tokenizer.pad_token_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941ae091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['fid', 'idx', 'content', 'label'],\n",
       "    num_rows: 53213\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "496dc7fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 23])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278,\n",
       "          50278, 50278, 50278,     0, 10118,  1621,    27, 50276,  2693,    39,\n",
       "            520, 29195, 50279,  1838, 20872,    27,  2693,    39,   520, 29195,\n",
       "            209, 50277],\n",
       "         [50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278,\n",
       "          50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278, 50278,\n",
       "              0, 14311,  4379, 50279,  1267,  1848,  2025,    27, 12965,  4379,\n",
       "            209, 50277],\n",
       "         [    0,   416,  1400, 42525, 50276,    53,  1719, 50276,  1235,  2759,\n",
       "          50279,    36,  7400,    27,    51,  1400, 42525,    61,    79, 19247,\n",
       "             27,    53,  1719,    61,    79,    59,  3123,    27,  1235,  2759,\n",
       "            209, 50277]]),\n",
       " tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,     0, 10118,  1621,    27, 50276,  2693,    39,\n",
       "            520, 29195, 50279,  1838, 20872,    27,  2693,    39,   520, 29195,\n",
       "            209, 50277],\n",
       "         [ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "              0, 14311,  4379, 50279,  1267,  1848,  2025,    27, 12965,  4379,\n",
       "            209, 50277],\n",
       "         [    0,   416,  1400, 42525, 50276,    53,  1719, 50276,  1235,  2759,\n",
       "          50279,    36,  7400,    27,    51,  1400, 42525,    61,    79, 19247,\n",
       "             27,    53,  1719,    61,    79,    59,  3123,    27,  1235,  2759,\n",
       "            209, 50277]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1],\n",
       "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from islab.aicup import collate_batch_with_prompt_template\n",
    "\n",
    "train_data = list(dataset['train'])\n",
    "train_dataloader = DataLoader(train_data, batch_size=3, shuffle=False, collate_fn=lambda batch: collate_batch_with_prompt_template(batch, tokenizer))\n",
    "titer = iter(train_dataloader)\n",
    "tks, labels, masks= next(titer)\n",
    "print(tks.shape)\n",
    "next(iter(titer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c3f0c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "<|endoftext|> 9364819.RAN\\nMINTANIA, JEFFRY \n",
      "\n",
      "####\n",
      "\n",
      " ID: 9364819.RAN\\nNAME: MINTANIA, JEFFRY <|END|>\n",
      "<|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|endoftext|> This is a sentence \n",
      "\n",
      "####\n",
      "\n",
      " PHI: NULL <|END|>\n"
     ]
    }
   ],
   "source": [
    "results = tokenizer(\n",
    "    [f\"{bos} 9364819.RAN\\\\nMINTANIA, JEFFRY {sep} ID: 9364819.RAN\\\\nNAME: MINTANIA, JEFFRY {eos}\",\n",
    "     f\"{bos} This is a sentence {sep} PHI: NULL {eos}\"],\n",
    "    padding=True\n",
    ")\n",
    "print(results['attention_mask'][0])\n",
    "print(results['attention_mask'][1])\n",
    "print(tokenizer.decode(results['input_ids'][0]))\n",
    "print(tokenizer.decode(results['input_ids'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ff9a120a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from islab.aicup import OpenDeidBatchSampler\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "bucket_train_dataloader = DataLoader(train_data,\n",
    "                                     batch_sampler=OpenDeidBatchSampler(train_data, BATCH_SIZE),\n",
    "                                     collate_fn=lambda batch: collate_batch_with_prompt_template(batch, tokenizer),\n",
    "                                     pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ae40ffe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50304, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50304, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "# the model config to which we add the special tokens\n",
    "config = AutoConfig.from_pretrained(plm,\n",
    "                                    bos_token_id=tokenizer.bos_token_id,\n",
    "                                    eos_token_id=tokenizer.eos_token_id,\n",
    "                                    pad_token_id=tokenizer.pad_token_id,\n",
    "                                    sep_token_id=tokenizer.sep_token_id,\n",
    "                                    output_hidden_states=False)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(plm, revision=\"step3000\", config=config)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fae1337b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoXForCausalLM(\n",
       "  (gpt_neox): GPTNeoXModel(\n",
       "    (embed_in): Embedding(50280, 512)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x GPTNeoXLayer(\n",
       "        (input_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (post_attention_layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "        (attention): GPTNeoXAttention(\n",
       "          (rotary_emb): RotaryEmbedding()\n",
       "          (query_key_value): Linear(in_features=512, out_features=1536, bias=True)\n",
       "          (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "        )\n",
       "        (mlp): GPTNeoXMLP(\n",
       "          (dense_h_to_4h): Linear(in_features=512, out_features=2048, bias=True)\n",
       "          (dense_4h_to_h): Linear(in_features=2048, out_features=512, bias=True)\n",
       "          (act): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (embed_out): Linear(in_features=512, out_features=50280, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "EPOCHS = 1 # CHANGE TO THE NUMBER OF EPOCHS YOU WANT\n",
    "optimizer = AdamW(model.parameters(),lr=3e-5) # YOU CAN ADJUST LEARNING RATE\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fec91d75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 1.5978602690043144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████| 1/1 [19:38<00:00, 1178.89s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm,trange\n",
    "# 模型儲存資料夾名稱\n",
    "model_name = \"xxx2\"\n",
    "# 模型儲存路徑\n",
    "model_dir = f\"C:/Users/user/OneDrive/Documents/資料探勘/DATA/First_Phase_Release(Correction)/{model_name}\"\n",
    "if not os.path.isdir(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "min_loss = 9999\n",
    "\n",
    "global_step = 0\n",
    "total_loss = 0\n",
    "\n",
    "model.train()\n",
    "for _ in trange(EPOCHS, desc=\"Epoch\"):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    # Training loop\n",
    "    predictions , true_labels = [], []\n",
    "\n",
    "    for step, (seqs, labels, masks) in enumerate(bucket_train_dataloader):\n",
    "        seqs = seqs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        masks = masks.to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(seqs, labels=labels, attention_mask=masks)\n",
    "        logits = outputs.logits\n",
    "        loss = outputs.loss\n",
    "        loss = loss.mean()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    avg_train_loss = total_loss / len(bucket_train_dataloader)\n",
    "    print(\"Average train loss: {}\".format(avg_train_loss))\n",
    "    torch.save(model.state_dict(), os.path.join(model_dir , 'GPT_Finial.pt'))\n",
    "    if avg_train_loss < min_loss:\n",
    "        min_loss = avg_train_loss\n",
    "        torch.save(model.state_dict(), os.path.join(model_dir , 'GPT_best.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4659c3e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
